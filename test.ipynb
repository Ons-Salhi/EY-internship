{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished importing\n"
     ]
    }
   ],
   "source": [
    "# importing libraries and packages for this project\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print('finished importing')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : login to linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished initializing a driver\n"
     ]
    }
   ],
   "source": [
    "# Access linkedin and login \n",
    "driver = webdriver.Edge()\n",
    "url = 'https://www.linkedin.com/login'\n",
    "driver.get(url)\n",
    "print('Finished initializing a driver')\n",
    "time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished importing login credentials\n",
      "Finished entering email\n",
      "Finished entering password\n",
      "Successfully logged in!\n"
     ]
    }
   ],
   "source": [
    "# Import username and password\n",
    "credential = open('login_credential.txt')\n",
    "line = credential.readlines()\n",
    "username = line[0]\n",
    "password = line[1]\n",
    "print('Finished importing login credentials')\n",
    "\n",
    "# Enter your username\n",
    "email_field = driver.find_element(By.ID,'username').send_keys(username + Keys.ENTER)\n",
    "print('Finished entering email')\n",
    "time.sleep(2)\n",
    "\n",
    "# Enter your password\n",
    "password_field = driver.find_element(By.NAME,'session_password').send_keys(password)\n",
    "print('Finished entering password')\n",
    "time.sleep(1)\n",
    "\n",
    "# Click login button\n",
    "login_field = driver.find_element(By.XPATH,'//*[@id=\"organic-div\"]/form/div[3]/button[1]')\n",
    "login_field.click()\n",
    "print('Successfully logged in!')\n",
    "time.sleep(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "search_query ='data science'\n",
    "profile_to_find = urllib.parse.quote(search_query)\n",
    "\n",
    "url = 'https://www.linkedin.com/search/results/people/?keywords='+profile_to_find+'&origin=SWITCH_SEARCH_VERTICAL&page='\n",
    "\n",
    "number_of_pages=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to page 1\n",
      "['https://www.linkedin.com/feed/?nis=true', 'https://www.linkedin.com/feed/?nis=true&', 'https://www.linkedin.com/mynetwork/?', 'https://www.linkedin.com/jobs/?', 'https://www.linkedin.com/messaging/?', 'https://www.linkedin.com/notifications/?', 'https://www.linkedin.com/in/eya-ghribi-217529212?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAADXTu_MBsbNOGcz1Bh1Tq7uLTgXzB9gi78k', 'https://www.linkedin.com/in/eya-ghribi-217529212?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAADXTu_MBsbNOGcz1Bh1Tq7uLTgXzB9gi78k', 'https://www.linkedin.com/in/eya-kaabachi?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAACorTUoBw9Iqdfwm6Li3V5Li8hxabbOIAlI', 'https://www.linkedin.com/in/eya-kaabachi?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAACorTUoBw9Iqdfwm6Li3V5Li8hxabbOIAlI', 'https://www.linkedin.com/in/haddadwalid?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAACkkzWgB6NoFgNJDVwznIFB3f5oAabuvtrE', 'https://www.linkedin.com/in/haddadwalid?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAACkkzWgB6NoFgNJDVwznIFB3f5oAabuvtrE', 'https://www.linkedin.com/in/wathek-mezni-926944187?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAACwZgC4BlJG6t1qF4v8Girpz4z8lHfbqBlI', 'https://www.linkedin.com/in/wathek-mezni-926944187?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAACwZgC4BlJG6t1qF4v8Girpz4z8lHfbqBlI', 'https://www.linkedin.com/in/ACoAADORjZIBpmDCTG0x9hsithKOJpidPDh4FL8?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAADORjZIBpmDCTG0x9hsithKOJpidPDh4FL8', 'https://www.linkedin.com/in/hedi-jarraya-2a4567190?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAACzu0twBFfxwKnDaBgUDu1ziJxHsYa9Z0pk', 'https://www.linkedin.com/in/hedi-jarraya-2a4567190?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAACzu0twBFfxwKnDaBgUDu1ziJxHsYa9Z0pk', 'https://www.linkedin.com/in/ons-salhi?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAD0GBA0BvKSBc_L5uwm4P5zT867CB46gIlU', 'https://www.linkedin.com/in/ons-salhi?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAD0GBA0BvKSBc_L5uwm4P5zT867CB46gIlU', 'https://www.linkedin.com/in/ACoAADORjZIBpmDCTG0x9hsithKOJpidPDh4FL8?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAADORjZIBpmDCTG0x9hsithKOJpidPDh4FL8', 'https://www.linkedin.com/in/eya-ghodhbeni-pro?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAC9dgGMBUiVhH_rAEDeEPYEVrFouXCGJP2k', 'https://www.linkedin.com/in/eya-ghodhbeni-pro?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAC9dgGMBUiVhH_rAEDeEPYEVrFouXCGJP2k', 'https://www.linkedin.com/in/ACoAADORjZIBpmDCTG0x9hsithKOJpidPDh4FL8?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAADORjZIBpmDCTG0x9hsithKOJpidPDh4FL8', 'https://www.linkedin.com/in/amine-saidi-78342a241?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAADwKqdMB4ZkQvj5U7dMgnLT153qwe_QeWIo', 'https://www.linkedin.com/in/amine-saidi-78342a241?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAADwKqdMB4ZkQvj5U7dMgnLT153qwe_QeWIo', 'https://www.linkedin.com/in/yasminedaly?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAADEUX3QBECv74IXPq7OawePVy8ao9ltu2zI', 'https://www.linkedin.com/in/yasminedaly?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAADEUX3QBECv74IXPq7OawePVy8ao9ltu2zI', 'https://www.linkedin.com/in/enoch-lee-6011511b3?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAADGtWyEBAMkmS53ZoXh5LF5YA_6GgHi0epo', 'https://www.linkedin.com/in/enoch-lee-6011511b3?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAADGtWyEBAMkmS53ZoXh5LF5YA_6GgHi0epo', 'https://www.linkedin.com/search/results/all/?keywords=data%20scientist&origin=RELATED_SEARCH_FROM_SRP', 'https://www.linkedin.com/search/results/all/?keywords=data%20analytics&origin=RELATED_SEARCH_FROM_SRP', 'https://www.linkedin.com/search/results/all/?keywords=data%20science%20manager&origin=RELATED_SEARCH_FROM_SRP', 'https://www.linkedin.com/search/results/all/?keywords=machine%20learning&origin=RELATED_SEARCH_FROM_SRP', 'https://www.linkedin.com/search/results/all/?keywords=software&origin=RELATED_SEARCH_FROM_SRP', 'https://www.linkedin.com/search/results/all/?keywords=data%20analyst&origin=RELATED_SEARCH_FROM_SRP']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "collections.Callable = collections.abc.Callable\n",
    "links=[]\n",
    "def extract_links(i):\n",
    "    url = 'https://www.linkedin.com/search/results/people/?keywords='+profile_to_find+'&origin=SWITCH_SEARCH_VERTICAL&page='+str(i)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    profiles = driver.find_elements(By.CLASS_NAME,'app-aware-link ')\n",
    "    profile_links = []\n",
    "    for profile in profiles:\n",
    "        profile_links.append(profile.get_attribute(\"href\"))\n",
    "    for link in profile_links:\n",
    "        links.append(link)\n",
    "for i in range(1,number_of_pages+1):\n",
    "    time.sleep(4)\n",
    "    extract_links(i)\n",
    "    print('going to page '+str(i))\n",
    "l=set(links)\n",
    "file = open('links.txt','w')\n",
    "for item in l:\n",
    "    if ('www.linkedin.com/in/' in item):\n",
    "\t    file.write(item+\"\\n\")\n",
    "file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m link:\n\u001b[0;32m      8\u001b[0m     driver\u001b[39m.\u001b[39mget(i)\n\u001b[1;32m----> 9\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m3\u001b[39;49m)\n\u001b[0;32m     10\u001b[0m     name \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39mfind_element(By\u001b[39m.\u001b[39mXPATH, \u001b[39m'\u001b[39m\u001b[39m//h1[@class=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-heading-xlarge inline t-24 v-align-middle break-words\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('test.txt','w',encoding='utf-8') as t:\n",
    "\n",
    "    with open('items.txt', 'r',encoding='utf-8') as f:\n",
    "        # Read the first line in the file\n",
    "        link = f.readlines()\n",
    "    for i in link:\n",
    "       \n",
    "        driver.get(i)\n",
    "        time.sleep(3)\n",
    "        name = driver.find_element(By.XPATH, '//h1[@class=\"text-heading-xlarge inline t-24 v-align-middle break-words\"]')\n",
    "        try:\n",
    "            for i in name:\n",
    "                t.write(i.get_attribute('innerHTML'))\n",
    "        except:\n",
    "            t.write(name.get_attribute('innerHTML'))\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        location = driver.find_elements(By.XPATH, '//span[@class=\"text-body-small inline t-black--light break-words\"]')\n",
    "        try:\n",
    "            for i in location:\n",
    "                t.write(i.get_attribute('innerHTML'))\n",
    "        except:\n",
    "            t.write(location.get_attribute('innerHTML'))   \n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        title = driver.find_elements(By.XPATH, '//div[@class=\"text-body-medium break-words\"]')\n",
    "        try:\n",
    "            for i in title:\n",
    "                t.write(i.get_attribute('innerHTML'))\n",
    "        except:\n",
    "            t.write(title.get_attribute('innerHTML'))\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "        experience_info = driver.find_elements(By.XPATH, '//section[@class=\"artdeco-card ember-view relative break-words pb3 mt2 \"]')\n",
    "        try:\n",
    "            for i in experience_info:\n",
    "                t.write(i.text)\n",
    "                t.write('#######')\n",
    "        except:\n",
    "            t.write(experience_info.get_attribute('outerHTML'))  \n",
    "            \n",
    "        t.write('\\n__________________________________________\\n')  \n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    output = [lines[0]]\n",
    "\n",
    "    # iterate over the lines and compare each line with its successor\n",
    "    for i in range(1, len(lines)):\n",
    "        if lines[i] != lines[i-1]:\n",
    "            # add the line to the output list if it's different from its predecessor\n",
    "            output.append(lines[i])\n",
    "\n",
    "# write the output list to a new file\n",
    "with open('final_info.txt', 'w',encoding='utf-8') as f:\n",
    "    f.writelines(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Open the text file with utf-8 encoding\n",
    "with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Split the data into sections delimited with \"__________________________________________\"\n",
    "sections = data.strip().split('\\n__________________________________________\\n')\n",
    "\n",
    "# Initialize a list to store all the data\n",
    "all_data = []\n",
    "\n",
    "# Loop through each section and extract the necessary information\n",
    "for section in sections:\n",
    "\n",
    "    # Split the section into individual lines\n",
    "    lines = section.strip().split('\\n')\n",
    "\n",
    "    # Extract the name, location, and title from the first four lines\n",
    "    name = lines[0]\n",
    "    location = lines[1]\n",
    "    title = lines[3]\n",
    "\n",
    "    # Loop through the rest of the lines and extract the section data\n",
    "    section_data = {}\n",
    "    i = 4\n",
    "    while i < len(lines):\n",
    "        # Check if the current line is a delimiter\n",
    "        if lines[i].startswith('#######'):\n",
    "            # Extract the section name and data\n",
    "            section_name = lines[i].lstrip('#').strip()\n",
    "            i += 1\n",
    "            section_end = len(lines)\n",
    "            for j in range(i, len(lines)):\n",
    "                if lines[j].startswith('#######'):\n",
    "                    section_end = j\n",
    "                    break\n",
    "            section_content = '\\n'.join(lines[i:section_end])\n",
    "            section_data[section_name] = section_content\n",
    "            i = section_end\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    # Loop through the section data and add each row to the all_data list\n",
    "    for section_name, section_content in section_data.items():\n",
    "        rows = section_content.strip().split('\\n')\n",
    "        for row in rows:\n",
    "            all_data.append([name, location, title, section_name, row])\n",
    "\n",
    "# Write the all_data list into a CSV file with utf-8 encoding\n",
    "with open('output.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Name', 'Location', 'Title', 'Section', 'Data'])\n",
    "    for row in all_data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Activity\n",
      "Activity\n",
      "1,497 followers\n",
      "Eya Ghribi commented on a post • 1d\n",
      "1d\n",
      "Congratulations !\n",
      "18\n",
      "8 comments\n",
      "Eya Ghribi commented on a post • 2w\n",
      "2w\n",
      "Congratulations ❤️❤️\n",
      "37\n",
      "12 comments\n",
      "Show all activity\n",
      " Experience\n",
      "Experience\n",
      "Data scientist\n",
      "TEAMWILL · Internship\n",
      "Dec 2022 - Present · 4 mos\n",
      "Gouvernorat Tunis, Tunisie\n",
      "Data Science Intern\n",
      "TEAMWILL · Internship\n",
      "Aug 2022 - Sep 2022 · 2 mos\n",
      "Tunis, Tunisie\n",
      "Stagiaire\n",
      "Valeo · Internship\n",
      "Jun 2021 - Jul 2021 · 2 mos\n",
      "Ben Arous, Tunisie\n",
      "Ben Arous, Tunisie\n",
      " Education\n",
      "Education\n",
      "Ecole Supérieure Privée d'Ingénierie et de Technologies - ESPRIT\n",
      "Diplôme d'ingénieur, Ingénierie informatique\n",
      "2020 - 2023\n",
      "Institut prèpartoire aux etudes d’ingenieur de Monastir\n",
      "2018 - 2020\n",
      "Lycèe pilote de KAIROUAN\n",
      "Baccalauréat, Mathématiques\n",
      "2014 - 2018\n",
      "2014 - 2018\n",
      " Skills\n",
      "Skills\n",
      "JavaScript\n",
      "4 endorsements\n",
      "HTML\n",
      "4 endorsements\n",
      "Administration Unix\n",
      "3 endorsements\n",
      "Show all 15 skills\n",
      " Interests\n",
      "Interests\n",
      "Top Voices\n",
      "Companies\n",
      "Groups\n",
      "Schools\n",
      "Clement Mihailescu\n",
      "Co-Founder & CEO of AlgoExpert | Ex-Google & Ex-Facebook Software Engineer | LinkedIn Top Voice\n",
      "469,084 followers\n",
      "Follow\n",
      " Experience\n",
      "Experience\n",
      "Data Science Intern\n",
      "United Nations · Internship\n",
      "Feb 2023 - Present · 2 mos\n",
      "Berkeley, California, United States\n",
      "Development Coordination Office - Database team\n",
      "Customer Engineering Intern\n",
      "Adobe · Internship\n",
      "May 2022 - Aug 2022 · 4 mos\n",
      "San Francisco, California, United States\n",
      "Skills: Data Validation · SQL\n",
      "Skills:Data Validation · SQL\n",
      "Data Analyst Intern\n",
      "Berkeley SkyDeck · Internship\n",
      "Jan 2022 - May 2022 · 5 mos\n",
      "Berkeley, California, United States\n",
      "Skills: KPI visualization · User segmentation analysis · Data Analytics\n",
      "Skills:KPI visualization · User segmentation analysis · Data Analytics\n",
      "CEO & Co-Founder\n",
      "Niche Stitch · Full-time\n",
      "Aug 2020 - Jan 2022 · 1 yr 6 mos\n",
      "대한민국\n",
      "Built a global distribution pipeline - Shelved on Don Quijote Japan, Watsons Taiwan, Incheon Airport Duty-Free store, and etc. \n",
      "Exported to 10 countries so far.\n",
      "Niche Stitch\n",
      "Pocket Fabric Perfume\n",
      "Translator/Interpreter\n",
      "Combined Forces Command · Full-time\n",
      "Feb 2017 - Sep 2017 · 8 mos\n",
      "South Korea\n",
      "South Korea\n",
      " Education\n",
      "Education\n",
      "University of California, Berkeley\n",
      "Bachelor's degree, Data Science\n",
      "2023\n",
      "Skills: C (Programming Language) · Java · Python · SQL\n",
      "Skills:C (Programming Language) · Java · Python · SQL\n",
      "Korea International Christian School\n",
      "High School Diploma\n",
      "2012 - 2015\n",
      "Activities and societies: KICS Student Council, Speaker @UWC Korea National Assembly Peace Forum, Compassion Translator, Head Chair @YMCA Model United Nations\n",
      "Kingswood College\n",
      "Feb 2009 - May 2012\n",
      "Activities and societies: U14A Rugby Team\n",
      "Activities and societies: U14A Rugby Team\n",
      " Skills\n",
      "Skills\n",
      "Data Validation\n",
      "Customer Engineering Intern at Adobe\n",
      "C (Programming Language)\n",
      "University of California, Berkeley\n",
      "KPI visualization\n",
      "Data Analyst Intern at Berkeley SkyDeck\n",
      "Show all 8 skills\n",
      " Courses\n",
      "Courses\n",
      "Principles and Techniques of Data Science\n",
      "DATA100\n",
      "Associated with University of California, Berkeley\n",
      "Production and Operations Management\n",
      "UGBA141\n",
      "Associated with University of California, Berkeley\n",
      "The Foundations of Data Science\n",
      "DATA8\n",
      "Associated with University of California, Berkeley\n",
      "Show all 4 courses\n",
      " Test scores\n",
      "Test scores\n",
      "OPIC\n",
      "Score: AL · Dec 2017\n",
      "TOEFL\n",
      "Score: 115 · Jul 2015\n",
      "Score: 115 · Jul 2015\n",
      " Organizations\n",
      "Organizations\n",
      "KOJOBS Berkeley\n",
      "President · Jan 2022 - Jun 2022\n",
      "Associated with University of California, Berkeley\n",
      "Associated with University of California, Berkeley\n",
      " Interests\n",
      "Interests\n",
      "Companies\n",
      "Groups\n",
      "Schools\n",
      "United Nations\n",
      "4,899,826 followers\n",
      "Follow\n",
      "Amazon Science\n",
      "292,664 followers\n",
      "Follow\n",
      "Show all 51 companies\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Open the text file with utf-8 encoding\n",
    "with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "people_data= data.strip().split('\\n__________________________________________\\n')\n",
    "\n",
    "for i in people_data:\n",
    "    lines=i.split('\\n')\n",
    "    name=lines[0]\n",
    "    location=lines[1]\n",
    "    title=lines[3]\n",
    "    length=len(lines)\n",
    "    sections = i.split(\"#######\")[1:-1]\n",
    "    for section in sections:\n",
    "        section_name, section_content = section.split('\\n', 1)\n",
    "        print(section_name, section_content.strip())\n",
    "        with open('f'+section_content.strip().split('\\n')[0]+'.txt', mode='w', newline='',encoding='utf-8') as f:\n",
    "            #section_content.strip().split('\\n')[0]     IS the section name\n",
    "            #section_content.strip()        is the section content\n",
    "\n",
    "            f.write(section_content.strip())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4ccad92a19cc290665e10b2e63bd0eff5d3aa8feca05732615aaf4cf68253bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
